shared_buffers setting
find relation between table and file in OS
select pg_relation_filepath('<table_name>');

==========
checkpoint_flush_after (default 0 pages)
checkpoint_timeout (in sec)
checkpoint_completion_target
infrequent checkpoints lead to IO spikes during checkpoints, degraded performance during checkpoint, higher recover time!
Background Writer writes dirty buffers based on algorithm, ensures that free buffers are available for use.
Checkpoint write all dirty buffers
Vacuum marks space as being available for reuse. No lock on table. Table will not shrink.
Vacuum full removes the deleted or updated records and reorders the table data. Lock on table.
lru=least recently used
FSM=free space management
\x
select * from pg_settings where name='bgwriter_delay';
==========
WAL use cases:
 * recovery
 * server startup
 * replication
fsync setting should be on, if off data inconsistency when power off.
===========
, you should at least configure the PostgreSQL buffer settings, the number of
connections, and logging.
log_min_duration_statement = 0 (millisec, default -1 disables logging)
--------------
TODO learn pgbadger
-------------
Vacuum test:
create table vacuum_test (id int) with (autovacuum_enabled = off);
insert into vacuum_test select * from generate_series(1,100000);
select pg_size_pretty(pg_relation_size('vacuum_test'));
update vacuum_test set id = id + 1;
select pg_size_pretty(pg_relation_size('vacuum_test'));
vacuum vacuum_test; 
--table will not shrink
-----------------
index bloat
table bloat (excessive dead tuples)=fragmented table
Find table bloat (where value n_dead_tup is high vs n_live_tup):
select * from pg_stat_user_tables;
--see n_dead_tup
settings 
autovacuum_naptime=60s --default, sleep time between autovacuum execution
autovacuum_max_workers=3
autovacuum_vacuum_scale_factor= 0.2 --start when 20% of data is changed
autovacuum_vacuum_threshold= 50 --list number of rows in the table for 20% rule
autovacuum_analyze_scale_factor = 0.1
autovacuum_analyze_threshold = 50 --10 % changed with at least 50 rows, then start stats for optimizer

select * from pg_settings where name='old_snapshot_threshold';
-1 (disabled)  limit the duration of a very slow or stuck transaction
Autovacuum is constantly running. Solution increase maintenance_work_mem and autovacuum_naptime;
vacuum_cost_delay
-------------------
create table test_indexing (id serial, name text);
insert into test_indexing (name) select 'bob' from generate_series (1,2500000); --2mln500 rows
insert into test_indexing (name) select 'alice' from generate_series (1,2500000);--2mln500 rows
-- 5 mln rows total
analyze;
\timing
select * from test_indexing where id=2;
time:118.787 ms
explain analyze select * from test_indexing where id=2;
->Parallel Seq Scan on test_indexing
--fix
create index idx_id on test_indexing (id);
select * from test_indexing where id=2;
time: 0.827 ms
explain analyze select * from test_indexing where id=2;
->Index Scan using idx_id on test_indexing
--see index space used
\di+
--see table space used
\dt+
index cardinality=low cardinality = a lot of dublicate values in column (ex gender)
index cardinality=high cardinality = none of little dublicate values in column (ex id)

==================================
Index recommendations:
When using parent-child relation (foreign keys), then create index on foreign keys column in child table.
--create parent table
create table orders (order_nr serial primary key, order_date date);
--create child table
create table items (
item_nr serial not null,
order_nr integer,
product_name varchar, 
descr varchar,
created_ts timestamp,
constraint fk_items foreign key (order)
references orders (order_nr)
match simple on update cascade on delete cascade);
--populate table with 1 mln (6zeros) rows, for each order 4 child rows
with order_rws as (insert into orders (order_nr, order_date)
select generate_series (1, 1000000) t, now() 
returning order_nr)
insert into items (item_nr, order_nr, product_name, descr, created_ts)
select generate_series(1,4) item_nr, order_nr, 'product'
repeat ('the description of product', 10), now()
from order_rws; 
\timing
select * from orders ord join items itm on ord.order_nr=itm.order_nr
where order_nr=12;
Time: 2531.233 ms (00:02.533)
create index idx_ord on items (order_nr);
select * from orders ord join items itm on ord.order_nr=itm.order_nr
where order_nr=12;
Time: 0.751
--Beter time execution with index on fk column, improved join performance!
--Show indexes
\di+
--Create partial index on rows with low cardinality (gender, nationality), so frequent values are > 25%
create index idx_name on test_indexing (name)
where name not in ('bob', 'alice');

-------------------
--Low correlation (<1) means that data is shuffled and the query on this column will be slower:
select tablename, attname, correlation from pg_stats where tablename in ('abc', 'def') order by 1, 2;
--Fix correlation (only for one index possible) with cluster command. Table will be locked.
cluster <table_name> using <index_name>;
-------------------
Fill factor suggestions (look at updates, not inserts!)
static table (the values of table never changed) 100%
table updated less ofter 95%
frequently updated table 70-90%
============================================================
--Create load on database using pgbench tool
--simulate load with 8 concurrent clients doing 25000 transactions in pgbench database
pgbench -S -c 8 -t 25000 <db_name>
select * from pg_stat_statements;
query column: text of executed query
stddev_time column: query has stable or not execution time (reason: data is not in cache)
shared_blks_hit column: data come from cache
shared_blks_read column: data come from disk
temp_blks_read column: if not zero add work_mem, maintenance_work_mem
temp_blks_written column: if not zero add work_mem, maintenance_work_mem
blk_read_time column: io operations, not zero with heavy load
blk_write_time column: io operation, not zero with heavy load
===============
--Top 10 time-consuming queries
SELECT round((100 * total_time / sum(total_time)                       
           OVER ())::numeric, 2) percent,                        
           round(total_time::numeric, 2) AS total,                  
           calls,                                                   
           round(mean_time::numeric, 2) AS mean,                    
           substring(query, 1, 200)                                  
 FROM  pg_stat_statements                                               
           ORDER BY total_time DESC                                               
           LIMIT 10
=================
parameter track_activity_query_size=1024 bytes 
long queries are cutoff , increase value to 32k (32786 bytes) postgresql.conf for hibernate applications
pg_stat_activity = query running now
pg_stat_statements = query in the past
==================
inspecting table statistics
pg_stat_all_tables (pg_stat_sys_tables, pg_stat_user_tables)
==================
Monitor how well vacuum is working? See n_live_tup and n_dead_tup columns
SELECT schemaname, relname,seq_scan, idx_scan,
       cast(idx_scan AS numeric) / (idx_scan + seq_scan)
       AS idx_scan_pct 
FROM pg_stat_user_tables 
       WHERE (idx_scan + seq_scan) > 0 ORDER BY idx_scan_pct;
--see column seq_scan and idx_scan

SELECT relname, seq_tup_read, idx_tup_fetch,
       cast(idx_tup_fetch AS numeric) / (idx_tup_fetch + seq_tup_read) 
       AS idx_tup_pct 
FROM pg_stat_user_tables 
       WHERE (idx_tup_fetch + seq_tup_read) > 0 ORDER BY idx_tup_pct;
--see how many tuples fetched
--if you find large tables in seq scan, then you find missing indexes
--so detect missing indexes, look at the top:
SELECT schemaname, relname, seq_scan, seq_tup_read, 
       seq_tup_read / seq_scan AS avg, idx_scan 
FROM   pg_stat_user_tables 
WHERE  seq_scan > 0 
ORDER BY seq_tup_read DESC  
LIMIT  25; 
========================
--Find tables for maintenance operation
-- append only table when ins_pct !=0, upd_pts=0, del_ptc=0
-- upd_ptc high, ins_ptc and del_ptc low -> reindex command needed
-- del_ptc high -> cluster command needed
SELECT relname,
       cast(n_tup_ins AS numeric) / (n_tup_ins + n_tup_upd + n_tup_del) AS ins_pct,
       cast(n_tup_upd AS numeric) / (n_tup_ins + n_tup_upd + n_tup_del) AS upd_pct,
       cast(n_tup_del AS numeric) / (n_tup_ins + n_tup_upd + n_tup_del) AS del_pct 
FROM pg_stat_user_tables 
       ORDER BY relname;
========================
--Find hot updates ptc (high=good) hot update=copy of row stays in same page, good for performance HOT=Heap Only Tuple=tuple that not referenced outside the table block, all is in the same block
SELECT relname,n_tup_upd, n_tup_hot_upd,
       cast(n_tup_hot_upd AS numeric) / n_tup_upd AS hot_pct 
FROM pg_stat_user_tables 
       WHERE n_tup_upd>0 ORDER BY hot_pct;
========================
--Find useless indexes
SELECT indexrelname,
       cast(idx_tup_read AS numeric) / idx_scan AS avg_tuples,
       idx_scan,idx_tup_read 
FROM pg_stat_user_indexes 
       WHERE idx_scan > 0;
========================

